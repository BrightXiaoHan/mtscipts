common:
  output_directory: .
  chunksize: 1000000

steps:
  - type: remove_duplicates
    parameters:
      inputs: [raw.zh, raw.en]
      outputs: [zh.dedup, en.dedup]

  - type: filter
    parameters:
      inputs: [zh.dedup, en.dedup]
      outputs: [zh.rules, en.rules]
      filterfalse: false
      filters:
        - LengthFilter:
            unit: [char, word]
            min_length: 1
            max_length: 200
        - CharacterScoreFilter:
            scripts: [Han, Latin]
            thresholds: [0.5, 0.8]
        - SimilarityFilter:
            unit: char
        - RegExpFilter:
            regexps: [".*[\\u4E00-\\u9FA5]+.*", ".*[A-Za-z]+.*"]
            accept_match: true
        - RegExpFilter:
            regexps: [".*[\\u4E00-\\u9FA5]+.*", "[^\\u4E00-\\u9FA5]+$"]
            accept_match: true

  - type: train_alignment
    parameters:
      src_data: zh.rules
      tgt_data: en.rules
      scores: align_score.jsonl
      output: align.priors
      parameters:
        model: 3
        src_tokenizer: [jieba, zh]
        tgt_tokenizer: [moses, en]

  - type: filter
    parameters:
      inputs: [zh.rules, en.rules]
      output: [train.zh, train.en]
      filters:
        - WordAlignFilter:
            src_threshold: 0
            tgt_threshold: 0
            model: 3
            priors: align.priors
            src_tokenizer: [jieba, zh]
            tgt_tokenizer: [moses, en]

  - type: filter
    parameters:
      inputs: [zh.rules, en.rules]
      output: [misalign.zh, misalign.en]
      filterfalse: true
      filters:
        - WordAlignFilter:
            src_threshold: 0
            tgt_threshold: 0
            model: 3
            priors: align.priors
            src_tokenizer: [jieba, zh]
            tgt_tokenizer: [moses, en]
